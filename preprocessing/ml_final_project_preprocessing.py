# -*- coding: utf-8 -*-
"""ML_Final_Project_Preprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nFxP98vZPPgkqCPqmqQ3B-UwJIAA3ANW
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import os
import glob
from pathlib import Path
import seaborn as sns

#Access all the folders in the parent directory
root_folder = r'/Users/bustergibson/documents/ml/pm25'
states = [name for name in os.listdir(root_folder) if os.path.isdir(os.path.join(root_folder, name))]
states

def get_csvs(state):
    full_path = root_folder + '/' + state
    files = os.listdir(full_path)
    list_of_csvs = []
    for file in files:
        if '.csv' in file:
            csv_path = full_path + '/' + file
            list_of_csvs.append(csv_path)
        else:
            pass
    return list_of_csvs

#pd.set_option('display.max_columns', None)
#pd.set_option('display.max_rows', None) 

#Create csvs with yearly averages of pm 2.5 per site per state
for state in states:
    csvs = get_csvs(state)
    df = pd.concat(map(pd.read_csv, csvs), ignore_index=True)
    df['DATE'] = pd.to_datetime(df['Date'], format='%m/%d/%Y')
    df['YEAR'] = df['DATE'].dt.year
    mean_df = df.groupby(['YEAR','Site ID','Site Name','STATE','SITE_LATITUDE','SITE_LONGITUDE'], as_index=False)['Daily Mean PM2.5 Concentration'].mean()
    table = pd.pivot_table(mean_df, values = 'Daily Mean PM2.5 Concentration',index = ['Site ID','Site Name','STATE','SITE_LATITUDE','SITE_LONGITUDE'],columns=['YEAR'])
    out_csv = r'/Users/bustergibson/documents/ml/pm25/%s/%s_PM25_Yearly_Averages.csv' % (state,state)
    mean_df.to_csv(out_csv)

#CSVs
ct_csv = r'/Users/bustergibson/documents/ml/pm25/Connecticut/Connecticut_PM25_Yearly_Averages.csv'
me_csv = r'/Users/bustergibson/documents/ml/pm25/Maine/Maine_PM25_Yearly_Averages.csv'
ma_csv = r'/Users/bustergibson/documents/ml/pm25/Massachusetts/Massachusetts_PM25_Yearly_Averages.csv'
nh_csv = r'/Users/bustergibson/documents/ml/pm25/New_Hampshire/New_Hampshire_PM25_Yearly_Averages.csv'
nj_csv = r'/Users/bustergibson/documents/ml/pm25/New_Jersey/New_Jersey_PM25_Yearly_Averages.csv'
ny_csv = r'/Users/bustergibson/documents/ml/pm25/New_York/New_York_PM25_Yearly_Averages.csv'
pa_csv = r'/Users/bustergibson/documents/ml/pm25/Pennsylvania/Pennsylvania_PM25_Yearly_Averages.csv'
ri_csv = r'/Users/bustergibson/documents/ml/pm25/Rhode_Island/Rhode_Island_PM25_Yearly_Averages.csv'
vt_csv = r'/Users/bustergibson/documents/ml/pm25/Vermont/Vermont_PM25_Yearly_Averages.csv'

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None) 
#pd.set_option('display.float_format', lambda x: '%.3f' % x)
#sites = r'/Users/bustergibson/documents/ml/PM25_Data_ALL_Points.shp'

# Read points from csv

#CT
ct_df = pd.read_csv(ct_csv)
ct_df = ct_df.dropna()
re_ct_df = ct_df.melt(id_vars=['Site Name','Site ID','STATE','SITE_LATITUDE', 'SITE_LONGITUDE'], 
                   value_vars=['2010','2011','2012','2013','2014','2015','2016','2017','2018','2019','2020','2021'])

re_ct_df = re_ct_df.rename({'variable': 'YEAR', 'value': 'PM25'}, axis=1)
re_ct_df

#ME
me_df = pd.read_csv(me_csv)
me_df = me_df.dropna()
re_me_df = me_df.melt(id_vars=['Site Name','Site ID','STATE','SITE_LATITUDE', 'SITE_LONGITUDE'], 
                   value_vars=['2010','2011','2012','2013','2014','2015','2016','2017','2018','2019','2020','2021'])

re_me_df = re_me_df.rename({'variable': 'YEAR', 'value': 'PM25'}, axis=1)
re_me_df


#MA
ma_df = pd.read_csv(ma_csv)
ma_df = ma_df.dropna()
re_ma_df = ma_df.melt(id_vars=['Site Name','Site ID','STATE','SITE_LATITUDE', 'SITE_LONGITUDE'], 
                   value_vars=['2010','2011','2012','2013','2014','2015','2016','2017','2018','2019','2020','2021'])

re_ma_df = re_ma_df.rename({'variable': 'YEAR', 'value': 'PM25'}, axis=1)
re_ma_df

#NH
nh_df = pd.read_csv(nh_csv)
nh_df = nh_df.dropna()
re_nh_df = nh_df.melt(id_vars=['Site Name','Site ID','STATE','SITE_LATITUDE', 'SITE_LONGITUDE'], 
                   value_vars=['2010','2011','2012','2013','2014','2015','2016','2017','2018','2019','2020','2021'])

re_nh_df = re_nh_df.rename({'variable': 'YEAR', 'value': 'PM25'}, axis=1)
re_nh_df

#NJ
nj_df = pd.read_csv(nj_csv)
nj_df = nj_df.dropna()
re_nj_df = nj_df.melt(id_vars=['Site Name','Site ID','STATE','SITE_LATITUDE', 'SITE_LONGITUDE'], 
                   value_vars=['2010','2011','2012','2013','2014','2015','2016','2017','2018','2019','2020','2021'])

re_nj_df = re_nj_df.rename({'variable': 'YEAR', 'value': 'PM25'}, axis=1)
re_nj_df

#NY
ny_df = pd.read_csv(ny_csv)
ny_df = ny_df.dropna()
re_ny_df = ny_df.melt(id_vars=['Site Name','Site ID','STATE','SITE_LATITUDE', 'SITE_LONGITUDE'], 
                   value_vars=['2010','2011','2012','2013','2014','2015','2016','2017','2018','2019','2020','2021'])

re_ny_df = re_ny_df.rename({'variable': 'YEAR', 'value': 'PM25'}, axis=1)
re_ny_df

#PA
pa_df = pd.read_csv(pa_csv)
pa_df = pa_df.dropna()
re_pa_df = pa_df.melt(id_vars=['Site Name','Site ID','STATE','SITE_LATITUDE', 'SITE_LONGITUDE'], 
                   value_vars=['2010','2011','2012','2013','2014','2015','2016','2017','2018','2019','2020','2021'])

re_pa_df = re_pa_df.rename({'variable': 'YEAR', 'value': 'PM25'}, axis=1)
re_pa_df

#RI
ri_df = pd.read_csv(ri_csv)
ri_df = ri_df.dropna()
re_ri_df = ri_df.melt(id_vars=['Site Name','Site ID','STATE','SITE_LATITUDE', 'SITE_LONGITUDE'], 
                   value_vars=['2010','2011','2012','2013','2014','2015','2016','2017','2018','2019','2020','2021'])

re_ri_df = re_ri_df.rename({'variable': 'YEAR', 'value': 'PM25'}, axis=1)
re_ri_df

#VT
vt_df = pd.read_csv(vt_csv)
vt_df = vt_df.dropna()
re_vt_df = vt_df.melt(id_vars=['Site Name','Site ID','STATE','SITE_LATITUDE', 'SITE_LONGITUDE'], 
                   value_vars=['2010','2011','2012','2013','2014','2015','2016','2017','2018','2019','2020','2021'])

re_vt_df = re_vt_df.rename({'variable': 'YEAR', 'value': 'PM25'}, axis=1)
re_vt_df

database = pd.concat([re_ct_df,re_me_df,re_ma_df,re_nh_df,re_nj_df,re_ny_df,re_pa_df,re_ri_df,re_vt_df],ignore_index=True)
print(len(database))
#result.sort_values(by=['YEAR'])
database.sort_values(by=['YEAR'])

import warnings
warnings.filterwarnings('ignore')

years = ['2010','2011','2012','2013','2014','2015','2016','2017','2018','2019','2020','2021']

#2010
year = '2010'

subset_2010 = database[database['YEAR'] == '2010']
coords = [(float(x),float(y)) for x, y in zip(subset_2010.SITE_LONGITUDE, subset_2010.SITE_LATITUDE)]
# Open the raster and store metadata
in_raster_2010 = r'/Users/bustergibson/documents/ml/aod/avg/aod_%s_avg.tif' % year
src_2010 = rasterio.open(in_raster_2010)
#Sample the raster at every point location and store values in DataFrame
field = 'AOD'
subset_2010[field] = [x[0] for x in src_2010.sample(coords)]

#2011
year = '2011'

subset_2011 = database[database['YEAR'] == '2011']
coords = [(float(x),float(y)) for x, y in zip(subset_2011.SITE_LONGITUDE, subset_2011.SITE_LATITUDE)]
# Open the raster and store metadata
in_raster_2011 = r'/Users/bustergibson/documents/ml/aod/avg/aod_%s_avg.tif' % year
src_2011 = rasterio.open(in_raster_2011)
#Sample the raster at every point location and store values in DataFrame
field = 'AOD'
subset_2011[field] = [x[0] for x in src_2011.sample(coords)]

#2012
year = '2012'

subset_2012 = database[database['YEAR'] == '2012']
coords = [(float(x),float(y)) for x, y in zip(subset_2012.SITE_LONGITUDE, subset_2012.SITE_LATITUDE)]
# Open the raster and store metadata
in_raster_2012 = r'/Users/bustergibson/documents/ml/aod/avg/aod_%s_avg.tif' % year
src_2012 = rasterio.open(in_raster_2012)
#Sample the raster at every point location and store values in DataFrame
field = 'AOD'
subset_2012[field] = [x[0] for x in src_2012.sample(coords)]

#2013
year = '2013'

subset_2013 = database[database['YEAR'] == '2013']
coords = [(float(x),float(y)) for x, y in zip(subset_2013.SITE_LONGITUDE, subset_2013.SITE_LATITUDE)]
# Open the raster and store metadata
in_raster_2013 = r'/Users/bustergibson/documents/ml/aod/avg/aod_%s_avg.tif' % year
src_2013 = rasterio.open(in_raster_2013)
#Sample the raster at every point location and store values in DataFrame
field = 'AOD'
subset_2013[field] = [x[0] for x in src_2013.sample(coords)]

#2014
year = '2014'

subset_2014 = database[database['YEAR'] == '2014']
coords = [(float(x),float(y)) for x, y in zip(subset_2014.SITE_LONGITUDE, subset_2014.SITE_LATITUDE)]
# Open the raster and store metadata
in_raster_2014 = r'/Users/bustergibson/documents/ml/aod/avg/aod_%s_avg.tif' % year
src_2014 = rasterio.open(in_raster_2014)
#Sample the raster at every point location and store values in DataFrame
field = 'AOD'
subset_2014[field] = [x[0] for x in src_2014.sample(coords)]

#2015
year = '2015'

subset_2015 = database[database['YEAR'] == '2015']
coords = [(float(x),float(y)) for x, y in zip(subset_2015.SITE_LONGITUDE, subset_2015.SITE_LATITUDE)]
# Open the raster and store metadata
in_raster_2015 = r'/Users/bustergibson/documents/ml/aod/avg/aod_%s_avg.tif' % year
src_2015 = rasterio.open(in_raster_2015)
#Sample the raster at every point location and store values in DataFrame
field = 'AOD'
subset_2015[field] = [x[0] for x in src_2015.sample(coords)]

#2016
year = '2016'

subset_2016 = database[database['YEAR'] == '2016']
coords = [(float(x),float(y)) for x, y in zip(subset_2016.SITE_LONGITUDE, subset_2016.SITE_LATITUDE)]
# Open the raster and store metadata
in_raster_2016 = r'/Users/bustergibson/documents/ml/aod/avg/aod_%s_avg.tif' % year
src_2016 = rasterio.open(in_raster_2016)
#Sample the raster at every point location and store values in DataFrame
field = 'AOD'
subset_2016[field] = [x[0] for x in src_2016.sample(coords)]

#2017
year = '2017'

subset_2017 = database[database['YEAR'] == '2017']
coords = [(float(x),float(y)) for x, y in zip(subset_2017.SITE_LONGITUDE, subset_2017.SITE_LATITUDE)]
# Open the raster and store metadata
in_raster_2017 = r'/Users/bustergibson/documents/ml/aod/avg/aod_%s_avg.tif' % year
src_2017 = rasterio.open(in_raster_2017)
#Sample the raster at every point location and store values in DataFrame
field = 'AOD'
subset_2017[field] = [x[0] for x in src_2017.sample(coords)]

#2018
year = '2018'

subset_2018 = database[database['YEAR'] == '2018']
coords = [(float(x),float(y)) for x, y in zip(subset_2018.SITE_LONGITUDE, subset_2018.SITE_LATITUDE)]
# Open the raster and store metadata
in_raster_2018 = r'/Users/bustergibson/documents/ml/aod/avg/aod_%s_avg.tif' % year
src_2018 = rasterio.open(in_raster_2018)
#Sample the raster at every point location and store values in DataFrame
field = 'AOD'
subset_2018[field] = [x[0] for x in src_2018.sample(coords)]

#2019
year = '2019'

subset_2019 = database[database['YEAR'] == '2019']
coords = [(float(x),float(y)) for x, y in zip(subset_2019.SITE_LONGITUDE, subset_2019.SITE_LATITUDE)]
# Open the raster and store metadata
in_raster_2019 = r'/Users/bustergibson/documents/ml/aod/avg/aod_%s_avg.tif' % year
src_2019 = rasterio.open(in_raster_2019)
#Sample the raster at every point location and store values in DataFrame
field = 'AOD'
subset_2019[field] = [x[0] for x in src_2019.sample(coords)]

#2020
year = '2020'

subset_2020 = database[database['YEAR'] == '2020']
coords = [(float(x),float(y)) for x, y in zip(subset_2020.SITE_LONGITUDE, subset_2020.SITE_LATITUDE)]
# Open the raster and store metadata
in_raster_2020 = r'/Users/bustergibson/documents/ml/aod/avg/aod_%s_avg.tif' % year
src_2020 = rasterio.open(in_raster_2020)
#Sample the raster at every point location and store values in DataFrame
field = 'AOD'
subset_2020[field] = [x[0] for x in src_2020.sample(coords)]

#2021
year = '2021'

subset_2021 = database[database['YEAR'] == '2021']
coords = [(float(x),float(y)) for x, y in zip(subset_2021.SITE_LONGITUDE, subset_2021.SITE_LATITUDE)]
# Open the raster and store metadata
in_raster_2021 = r'/Users/bustergibson/documents/ml/aod/avg/aod_%s_avg.tif' % year
src_2021 = rasterio.open(in_raster_2021)
#Sample the raster at every point location and store values in DataFrame
field = 'AOD'
subset_2021[field] = [x[0] for x in src_2021.sample(coords)]

complete_df= pd.concat([subset_2010,subset_2011,subset_2012,subset_2013,
                        subset_2014,subset_2015,subset_2016,subset_2017,
                        subset_2018,subset_2019,subset_2020,subset_2021])
complete_df

complete_df.to_csv(r'/Users/bustergibson/documents/ml/pm25_aod_complete.csv')

#EPA points
year = '2021'
epa_points = r'/Users/bustergibson/documents/ml/EPA_CBSA_2021_Data.csv'
epa_df = pd.read_csv(epa_points)
#epa_df
coords = [(float(x),float(y)) for x, y in zip(epa_df.LONG, epa_df.LAT)]
# Open the raster and store metadata
in_raster_2021 = r'/Users/bustergibson/documents/ml/aod/avg/aod_%s_avg.tif' % year
src_2021 = rasterio.open(in_raster_2021)
#Sample the raster at every point location and store values in DataFrame
field = 'AOD'
epa_df[field] = [x[0] for x in src_2021.sample(coords)]
epa_df.to_csv(r'/Users/bustergibson/documents/ml/EPA_CBSA_2021_Data_w_AOD.csv')

